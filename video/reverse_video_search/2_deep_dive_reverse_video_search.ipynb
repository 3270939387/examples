{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8347485f",
   "metadata": {},
   "source": [
    "# Deep Dive Reverse Video Search\n",
    "\n",
    "In the [previous tutorial](./1_reverse_video_search_engine.ipynb), we've learnt how to build a reverse video search engine. Now let's make the solution more feasible in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569571ec",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's recall preparation steps first:\n",
    "1. Install packages\n",
    "2. Prepare data\n",
    "3. Start milvus\n",
    "\n",
    "### Install packages\n",
    "\n",
    "Make sure you have installed required python packages:\n",
    "\n",
    "| package |\n",
    "| -- |\n",
    "| pymilvus |\n",
    "| towhee |\n",
    "| towhee.models |\n",
    "| pillow |\n",
    "| ipython |\n",
    "| fastapi |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d8e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install -q pymilvus towhee towhee.models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef6b1a",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "This tutorial will use a small data extracted from [Kinetics400](https://www.deepmind.com/open-source/kinetics). You can download the subset from [Github](https://github.com/towhee-io/examples/releases/download/data/reverse_video_search.zip). \n",
    "\n",
    "The data is organized as follows:\n",
    "- **train:** candidate videos, 20 classes, 10 videos per class (200 in total)\n",
    "- **test:** query videos, same 20 classes as train data, 1 video per class (20 in total)\n",
    "- **reverse_video_search.csv:** a csv file containing an ***id***, ***path***, and ***label*** for each video in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54568b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -L https://github.com/towhee-io/examples/releases/download/data/reverse_video_search.zip -O\n",
    "! unzip -q -o reverse_video_search.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171f2e7",
   "metadata": {},
   "source": [
    "For later steps to easier get videos & measure results, we build some helpful functions in advance:\n",
    "- **ground_truth:** get ground-truth video ids for the query video by its path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1b0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./reverse_video_search.csv')\n",
    "\n",
    "id_video = df.set_index('id')['path'].to_dict()\n",
    "label_ids = {}\n",
    "for label in set(df['label']):\n",
    "    label_ids[label] = list(df[df['label']==label].id)\n",
    "    \n",
    "\n",
    "def ground_truth(path):\n",
    "    label = path.split('/')[-2]\n",
    "    return label_ids[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e98f62",
   "metadata": {},
   "source": [
    "### Start Milvus\n",
    "\n",
    "Before getting started with the engine, we also need to get ready with Milvus. Please make sure that you have started a Milvus service ([Milvus Guide](https://milvus.io/docs/v2.0.x/install_standalone-docker.md)).\n",
    "Here we prepare a function to work with a Milvus collection with the following parameters:\n",
    "- [L2 distance metric](https://milvus.io/docs/v2.0.x/metric.md#Euclidean-distance-L2)\n",
    "- [IVF_FLAT index](https://milvus.io/docs/v2.0.x/index.md#IVF_FLAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4fbffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "def create_milvus_collection(collection_name, dim):\n",
    "    connections.connect(host='localhost', port='19530')\n",
    "    \n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, descrition='ids', is_primary=True, auto_id=False),\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, descrition='embedding vectors', dim=dim)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description='deep dive reverse video search')\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    # create IVF_FLAT index for collection.\n",
    "    index_params = {\n",
    "        'metric_type':'L2',\n",
    "        'index_type':\"IVF_FLAT\",\n",
    "        'params':{\"nlist\": 400}\n",
    "    }\n",
    "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d8e66",
   "metadata": {},
   "source": [
    "### Build Engine\n",
    "\n",
    "Now we are ready to build a reverse-video-search engine. Here we show an engine built with `MVIT model` and its performance to make comparasion later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d015dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total insert time: 169.33s\n",
      "Total number of inserted data is 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_hit_ratio</th>\n",
       "      <th>mean_average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mvit</th>\n",
       "      <td>0.785</td>\n",
       "      <td>0.864911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_hit_ratio  mean_average_precision\n",
       "mvit           0.785                0.864911"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total search time: 21.16s\n"
     ]
    }
   ],
   "source": [
    "import towhee\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "collection = create_milvus_collection('mvit', 768)\n",
    "\n",
    "dc = (\n",
    "    towhee.read_csv('reverse_video_search.csv')\n",
    "      .runas_op['id', 'id'](func=lambda x: int(x))\n",
    "      .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 32})\n",
    "      .video_classification['frames', 'vec'].pytorchvideo(\n",
    "            model_name='mvit_base_32x3', predict=False, skip_preprocess=True)\n",
    "      .tensor_normalize['vec', 'vec']()\n",
    "      .to_milvus['id', 'vec'](collection=collection, batch=10)\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Total insert time: %.2fs'%(end-start))\n",
    "print('Total number of inserted data is {}.'.format(collection.num_entities))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "benchmark = (\n",
    "    towhee.glob['path']('./test/*/*.mp4')\n",
    "        .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 32})\n",
    "        .video_classification['frames', 'vec'].pytorchvideo(\n",
    "            model_name='mvit_base_32x3', predict=False, skip_preprocess=True)\n",
    "        .tensor_normalize['vec', 'vec']()\n",
    "        .milvus_search['vec', 'result'](collection=collection, limit=10)\n",
    "        .runas_op['path', 'ground_truth'](func=ground_truth)\n",
    "        .runas_op['result', 'result'](func=lambda res: [x.id for x in res])\n",
    "        .with_metrics(['mean_hit_ratio', 'mean_average_precision'])\n",
    "        .evaluate['ground_truth', 'result']('mvit')\n",
    "        .report()\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Total search time: %.2fs'%(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d78601",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "In production, memory consumption is always a major concern, which can by relieved by minimizing the embedding dimension. Random projection is a dimensionality reduction method for a set vectors in Euclidean space. Since this method is fast and requires no training, we'll try this technique and compare performance with MVIT model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474daba",
   "metadata": {},
   "source": [
    "First let's get a quick look at the engine performance without dimension reduction. The embedding dimension is 768."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca23c3e",
   "metadata": {},
   "source": [
    "To reduce dimension, we can apply a projection matrix in proper size to each original embedding. We can just add an operator `.runas_op['vec', 'vec'](func=lambda x: np.dot(x, projection_matrix))` right after an video embedding is generated. Let's see how's the engine performance with embedding dimension down to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7343f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total insert time: 162.57s\n",
      "Total number of inserted data is 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_hit_ratio</th>\n",
       "      <th>mean_average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mvit_128</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.845302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean_hit_ratio  mean_average_precision\n",
       "mvit_128           0.775                0.845302"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total search time: 19.39s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "projection_matrix = np.random.normal(scale=1.0, size=(768, 128))\n",
    "\n",
    "# def dim_reduce(vec):\n",
    "#     return np.dot(vec, projection_matrix)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "collection = create_milvus_collection('mvit_128', 128)\n",
    "\n",
    "dc = (\n",
    "    towhee.read_csv('reverse_video_search.csv')\n",
    "      .runas_op['id', 'id'](func=lambda x: int(x))\n",
    "      .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 32})\n",
    "      .video_classification['frames', 'vec'].pytorchvideo(\n",
    "            model_name='mvit_base_32x3', predict=False, skip_preprocess=True)\n",
    "      .runas_op['vec', 'vec'](func=lambda x: np.dot(x, projection_matrix))\n",
    "      .tensor_normalize['vec', 'vec']()\n",
    "      .to_milvus['id', 'vec'](collection=collection, batch=10)\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Total insert time: %.2fs'%(end-start))\n",
    "print('Total number of inserted data is {}.'.format(collection.num_entities))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "benchmark = (\n",
    "    towhee.glob['path']('./test/*/*.mp4')\n",
    "        .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 32})\n",
    "        .video_classification['frames', 'vec'].pytorchvideo(\n",
    "            model_name='mvit_base_32x3', predict=False, skip_preprocess=True)\n",
    "        .runas_op['vec', 'vec'](func=lambda x: np.dot(x, projection_matrix))\n",
    "        .tensor_normalize['vec', 'vec']()\n",
    "        .milvus_search['vec', 'result'](collection=collection, limit=10)\n",
    "        .runas_op['path', 'ground_truth'](func=ground_truth)\n",
    "        .runas_op['result', 'result'](func=lambda res: [x.id for x in res])\n",
    "        .with_metrics(['mean_hit_ratio', 'mean_average_precision'])\n",
    "        .evaluate['ground_truth', 'result']('mvit_128')\n",
    "        .report()\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Total search time: %.2fs'%(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c999b",
   "metadata": {},
   "source": [
    "It's surprising that the performance is not affected obviously. Both mHR and mAP descrease by 0.01 while the embedding size are reduced by 6 times (dimension from 768 to 128)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04bf0ab",
   "metadata": {},
   "source": [
    "## Parallel Execution\n",
    "\n",
    "We are able to enable parallel execution by simply calling `set_parallel` within the pipeline. It tells Towhee to process the data in parallel. The code below enables parallel execution on the above example. It shows that the execution speeds up by 25% for a data size of 200 videos. If you use a larger data, the improvement by parallel execution should be more obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1961c437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total insert time: 125.62s\n",
      "Total number of inserted data is 200.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "collection = create_milvus_collection('mvit', 768)\n",
    "\n",
    "dc = (\n",
    "    towhee.read_csv('reverse_video_search.csv')\n",
    "      .runas_op['id', 'id'](func=lambda x: int(x))\n",
    "      .set_parallel(5)\n",
    "      .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 32})\n",
    "      .video_classification['frames', 'vec'].pytorchvideo(\n",
    "            model_name='mvit_base_32x3', predict=False, skip_preprocess=True)\n",
    "      .tensor_normalize['vec', 'vec']()\n",
    "      .to_milvus['id', 'vec'](collection=collection, batch=10)\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Total insert time: %.2fs'%(end-start))\n",
    "print('Total number of inserted data is {}.'.format(collection.num_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4061d0",
   "metadata": {},
   "source": [
    "## Exception Safe\n",
    "\n",
    "When we have large-scale data, there may be some bad data that will cause errors. Typically, the users don't want such errors to break the system in production. Therefore, the pipeline should continue to process the rest of the videos and report broken ones.\n",
    "\n",
    "Towhee supports an `exception-safe` execution mode that allows the pipeline to continue on exceptions and represent the exceptions with `Empty` values. The user can choose how to deal with the empty values at the end of the pipeline. During the query below, there are 4 files in total under the `exception` folder, one of them is broken. With `exception-safe`, it will print the ERROR but NOT terminate the process. As you can see from results, `drop_empty` deletes empty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aa5e3f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "2022-06-01 21:35:11,519 - 140445661360704 - video_decoder.py-video_decoder:121 - ERROR: moov atom not found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border-collapse: collapse;\"><tr><th style=\"text-align: center; font-size: 130%; border: none;\">path</th> <th style=\"text-align: center; font-size: 130%; border: none;\">res_path</th></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./exception/kDuAS29BCwk.mp4</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[./train/chopping_wood/Fq-N6hpOTB...,./train/chopping_wood/tVfchvUzas...,./train/chopping_wood/aXO-sNzhiE...,./train/chopping_wood/WjIEsPkw5R...,...] len=10</td></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./exception/ty4UQlowp0c.mp4</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[./train/eating_carrots/9OZhQqMhX...,./train/eating_carrots/y1U6Z2ZYQ...,./train/eating_carrots/bTCznQiu0...,./train/eating_carrots/V7DUq0JJn...,...] len=10</td></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./exception/rJu8mSNHX_8.mp4</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[./train/pumping_fist/FGZ_lEaHCws...,./train/eating_carrots/Ou1w86qEr...,./train/eating_carrots/WkwzsrDd-...,./train/eating_carrots/bTCznQiu0...,...] len=10</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    "    towhee.glob['path']('./exception/*')\n",
    "          .exception_safe()\n",
    "          .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 32})\n",
    "          .video_classification['frames', 'vec'].pytorchvideo(\n",
    "              model_name='mvit_base_32x3', predict=False, skip_preprocess=True)\n",
    "          .tensor_normalize['vec', 'vec']()\n",
    "          .milvus_search['vec', 'result'](collection=collection, limit=10)\n",
    "          .runas_op['result', 'res_path'](func=lambda res: [id_video[x.id] for x in res])\n",
    "          .drop_empty()\n",
    "          .select['path', 'res_path']()\n",
    "          .show()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg-dev",
   "language": "python",
   "name": "mg-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
