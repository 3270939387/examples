{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868d7f57",
   "metadata": {},
   "source": [
    "# Dive Deep Text-Video Retrieval Engine\n",
    "\n",
    "In the [previous tutorial](./1_text_video_retrieval_engine.ipynb), we've learnt how to build a text-video retrieval engine. Now let's make the solution more feasible in production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157044a8",
   "metadata": {},
   "source": [
    "## Preparation  \n",
    "\n",
    "Let's recall preparation steps first:\n",
    "1. Install packages\n",
    "2. Prepare data\n",
    "3. Start milvus\n",
    "\n",
    "### Install packages\n",
    "\n",
    "Make sure you have installed required python packages:\n",
    "\n",
    "| package |\n",
    "| -- |\n",
    "| pymilvus |\n",
    "| towhee |\n",
    "| towhee.models |\n",
    "| pillow |\n",
    "| ipython |\n",
    "| fastapi |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc9560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m pip install -q pymilvus towhee towhee.models pillow ipython fastapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891a4e6",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "First, we need to prepare the dataset and Milvus environment.   \n",
    "\n",
    "[MSR-VTT (Microsoft Research Video to Text)](https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/) is a dataset for the open domain video captioning, which consists of 10,000 video clips.  \n",
    "\n",
    "Download the MSR-VTT-1kA test set from [google drive](https://drive.google.com/file/d/1cuFpHiK3jV9cZDKcuGienxTg1YQeDs-w/view?usp=sharing) and unzip it, which contains just 1k videos.  \n",
    "And the video captions text sentence information is in ./MSRVTT_JSFUSION_test.csv.\n",
    "\n",
    "The data is organized as follows:\n",
    "- **test_1k_compress:** 1k compressed test videos in MSR-VTT-1kA.\n",
    "- **MSRVTT_JSFUSION_test.csv:** a csv file containing an ***key,vid_key,video_id,sentence***, for each video and caption text.\n",
    "\n",
    "Let's take a quick look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478b8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl -L https://github.com/towhee-io/examples/releases/download/data/text_video_search.zip -O\n",
    "# ! unzip -q -o text_video_search.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d0e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all test set is 1000\n",
      "random sample 1000 examples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# raw_video_path = './test_1k_compress' # 1k test video path.\n",
    "raw_video_path = os.path.join(os.path.abspath('.'), './test_1k_compress')\n",
    "\n",
    "test_csv_path = './MSRVTT_JSFUSION_test.csv' # 1k video caption csv.\n",
    "\n",
    "test_sample_csv_path = './MSRVTT_JSFUSION_test_sample.csv'\n",
    "\n",
    "sample_num = 1000 # you can change this sample_num to be smaller, so that this notebook will be faster.\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print('length of all test set is {}'.format(len(test_df)))\n",
    "sample_df = test_df.sample(sample_num, random_state=42)\n",
    "\n",
    "sample_df['video_path'] = sample_df.apply(lambda x:os.path.join(raw_video_path, x['video_id']) + '.mp4', axis=1)\n",
    "\n",
    "sample_df.to_csv(test_sample_csv_path)\n",
    "print('random sample {} examples'.format(sample_num))\n",
    "\n",
    "df = pd.read_csv(test_sample_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4911f0",
   "metadata": {},
   "source": [
    "Define some helper function to convert video to gif so that we can have a look at these video-text pairs.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdea96",
   "metadata": {},
   "source": [
    "Take a look at the ground-truth video-text pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57038b4",
   "metadata": {},
   "source": [
    "### Start Milvus\n",
    "\n",
    "Before getting started, please make sure you have [installed milvus](https://milvus.io/docs/v2.0.x/install_standalone-docker.md). Let's first create a `video retrieval` collection that uses the [L2 distance metric](https://milvus.io/docs/v2.0.x/metric.md#Euclidean-distance-L2) and an [IVF_FLAT index](https://milvus.io/docs/v2.0.x/index.md#IVF_FLAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84807c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "connections.connect(host='127.0.0.1', port='19530')\n",
    "\n",
    "def create_milvus_collection(collection_name, dim):\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, descrition='ids', is_primary=True, auto_id=False),\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, descrition='embedding vectors', dim=dim)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description='video retrieval')\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    # create IVF_FLAT index for collection.\n",
    "    index_params = {\n",
    "        'metric_type':'L2', #IP\n",
    "        'index_type':\"IVF_FLAT\",\n",
    "        'params':{\"nlist\":2048}\n",
    "    }\n",
    "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e2a48",
   "metadata": {},
   "source": [
    "## Parallel Execution\n",
    "\n",
    "We are able to enable parallel execution by simply calling `set_parallel` within the pipeline. It tells Towhee to process the data in parallel. The code below enables parallel execution on the above example. It shows that the execution speeds up by 13% for a data size of 1000 videos(1min 9s in this notebook vs 1min 19s in [previous tutorial](./1_text_video_retrieval_engine.ipynb)) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b74139",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 59s, sys: 1h 30min 3s, total: 1h 59min 3s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import towhee\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:2'\n",
    "# device = 'cpu'\n",
    "\n",
    "collection = create_milvus_collection('text_video_retrieval', 512)\n",
    "\n",
    "dc = (\n",
    "    towhee.read_csv(test_sample_csv_path)\n",
    "      .runas_op['video_id', 'id'](func=lambda x: int(x[-4:]))\n",
    "      .set_parallel(2)\n",
    "      .video_decode.ffmpeg['video_path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 12}) \\\n",
    "      .runas_op['frames', 'frames'](func=lambda x: [y for y in x]) \\\n",
    "      .video_text_embedding.clip4clip['frames', 'vec'](model_name='clip_vit_b32', modality='video', device=device) \\\n",
    "      .to_milvus['id', 'vec'](collection=collection, batch=30)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b490c0",
   "metadata": {},
   "source": [
    "## Exception Safe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951f992",
   "metadata": {},
   "source": [
    "Let's build an `exception_df` with a exception value, you can change the video_path to an error path, which will cause a exception in normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e8b8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>vid_key</th>\n",
       "      <th>video_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>521</td>\n",
       "      <td>ret521</td>\n",
       "      <td>msr7579</td>\n",
       "      <td>video7579</td>\n",
       "      <td>a girl wearing red top and black trouser is pu...</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>737</td>\n",
       "      <td>ret737</td>\n",
       "      <td>msr7725</td>\n",
       "      <td>video7725</td>\n",
       "      <td>young people sit around the edges of a room cl...</td>\n",
       "      <td>/home/data1/zhangchen_workspace/video-retrieva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>740</td>\n",
       "      <td>ret740</td>\n",
       "      <td>msr9258</td>\n",
       "      <td>video9258</td>\n",
       "      <td>a person is using a phone</td>\n",
       "      <td>/home/data1/zhangchen_workspace/video-retrieva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>660</td>\n",
       "      <td>ret660</td>\n",
       "      <td>msr7365</td>\n",
       "      <td>video7365</td>\n",
       "      <td>cartoon people are eating at a restaurant</td>\n",
       "      <td>/home/data1/zhangchen_workspace/video-retrieva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>411</td>\n",
       "      <td>ret411</td>\n",
       "      <td>msr8068</td>\n",
       "      <td>video8068</td>\n",
       "      <td>a woman on a couch talks to a a man</td>\n",
       "      <td>/home/data1/zhangchen_workspace/video-retrieva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     key  vid_key   video_id  \\\n",
       "0         521  ret521  msr7579  video7579   \n",
       "1         737  ret737  msr7725  video7725   \n",
       "2         740  ret740  msr9258  video9258   \n",
       "3         660  ret660  msr7365  video7365   \n",
       "4         411  ret411  msr8068  video8068   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  a girl wearing red top and black trouser is pu...   \n",
       "1  young people sit around the edges of a room cl...   \n",
       "2                          a person is using a phone   \n",
       "3          cartoon people are eating at a restaurant   \n",
       "4                a woman on a couch talks to a a man   \n",
       "\n",
       "                                          video_path  \n",
       "0                                                666  \n",
       "1  /home/data1/zhangchen_workspace/video-retrieva...  \n",
       "2  /home/data1/zhangchen_workspace/video-retrieva...  \n",
       "3  /home/data1/zhangchen_workspace/video-retrieva...  \n",
       "4  /home/data1/zhangchen_workspace/video-retrieva...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exception_df = pd.read_csv(test_sample_csv_path)\n",
    "exception_df.loc[0, 'video_path'] = 666\n",
    "print(type(exception_df.loc[0, 'video_path']))\n",
    "exception_csv_path = './MSRVTT_JSFUSION_test_exception.csv'\n",
    "exception_df.to_csv(exception_csv_path)\n",
    "exception_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73255bbe",
   "metadata": {},
   "source": [
    "Similarly, when we have large-scale data, there may be some bad data that will cause errors. Typically, the users don't want such errors to break the system in production. Therefore, the pipeline should continue to process the rest of the videos and report broken ones.\n",
    "\n",
    "Towhee supports an `exception-safe` execution mode that allows the pipeline to continue on exceptions and represent the exceptions with `Empty` values. The user can choose how to deal with the empty values at the end of the pipeline. During the query below, there are 4 files in total under the `exception` folder, one of them is broken. With `exception-safe`, it will print the ERROR but NOT terminate the process. As you can see from results, `drop_empty` deletes empty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16d7701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 36s, sys: 50min 1s, total: 1h 7min 37s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import towhee\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:2'\n",
    "# device = 'cpu'\n",
    "\n",
    "collection = create_milvus_collection('text_video_retrieval', 512)\n",
    "\n",
    "dc = (\n",
    "    towhee.read_csv(exception_csv_path)\n",
    "      .exception_safe()\n",
    "      .runas_op['video_id', 'id'](func=lambda x: int(x[-4:]))\n",
    "      .video_decode.ffmpeg['video_path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 12}) \\\n",
    "      .runas_op['frames', 'frames'](func=lambda x: [y for y in x]) \\\n",
    "      .video_text_embedding.clip4clip['frames', 'vec'](model_name='clip_vit_b32', modality='video', device=device) \\\n",
    "      .drop_empty()\n",
    "      .to_milvus['id', 'vec'](collection=collection, batch=30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53666798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inserted data is 999, while sample_num is 1000. The exception data is dropped.\n"
     ]
    }
   ],
   "source": [
    "print('Total number of inserted data is {}, while sample_num is {}. The exception data is dropped.'.format(collection.num_entities, sample_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3708ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}